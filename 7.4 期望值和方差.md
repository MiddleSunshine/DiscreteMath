# 7.4 期望值和方差

## 期望值

$$
E(X)=\sum_{s \in S}p(s)X(s)
$$

例如一个点数从1到6的骰子，其投掷一次的期望值是
$$
E(X)=1 \cdot \frac{1}{6}+2 \cdot \frac{1}{6}+3 \cdot \frac{1}{6}+4 \cdot \frac{1}{6}+5 \cdot \frac{1}{6}+6 \cdot \frac{1}{6}
$$
不用神话期望值，其本质就是对应的值乘以概率的和，所有的值要布满样本空间。

-----------

#### 随机变量与期望值

随机变量这个词比较迷惑性，可能是英译中的时候搞出来的，所以这里可以将其理解为函数。

将原本的样本空间中的字集经过函数处理后所得到的值，比如掷两个骰子，两个骰子的和。原本的样本空间和其所对应的函数处理结果如下所示，函数使用X表示：

```mathematica
X((1,1))=2
X((1,2))=X((2,1))=3
X((1,3))=X((3,1))=X((2,2))=4
X((1,4))=X((4,1))=X((2,3))=X((3,2))=5
X((1,5))=X((5,1))=X((2,4))=X((4,2))=X((3,3))=6
X((1,6))=X((6,1))=X((2,5))=X((5,2))=X((3,4))=X((4,3))=7
X((2,6))=X((6,2))=X((3,5))=X((5,3))=X((4,4))=8
X((3,6))=X((6,3))=X((4,5))=X((5,4))=9
X((4,6))=X((6,4))=X((5,5))=10
X((5,6))=X((6,5))=11
X((6,6))=12
```

其中原本的样本空间`(1,1),(1,2),(2,1)`经过函数处理后，构成了X的样本空间`X(S)`。
$$
(r,p(X=r))，代表r \in X(S)，p(X=r)代表X=r的概率。
$$

在上面的基础上，问投出的值的概率各是多少？

```mathematica
p(X=2)=p(X=12)=1/36
p(X=3)=p(X=11)=2/36
p(X=4)=p(X=10)=3/36
p(X=5)=p(X=9)=4/36
p(X=6)=p(x=8)=5/36
p(X=7)=6/36
```

这里的`p(X=2)`代表上面两个骰子的和为2的情况，看到只有1种，`X((1,1))`，`p(X=12)`同理，`X((6,6))`。

**随机变量其实本质就是在之前所有样本空间的基础上，使用函数，修改p(X=r)时，r的值。**

投出的值的期望值的计算就按照期望值的定义走就行了：
$$
E(X)=2 \cdot \frac{1}{36}+3 \cdot \frac{2}{36}+ 4 \cdot \frac{3}{36}+ 5 \cdot \frac{4}{36}+6 \cdot \frac{5}{36}+7 \cdot \frac{6}{36}\\
+12 \cdot \frac{1}{36}+11 \cdot \frac{2}{36}+ 10 \cdot \frac{3}{36}+ 9 \cdot \frac{4}{36}+8 \cdot \frac{5}{36} \\
=7
$$

#### 期望值的线性性质

在确定结果的情况下，可不可以试试单独计算两个骰子的期望值，然后相加呢？
$$
E(X_1)=1 \cdot \frac{1}{6}+2 \cdot \frac{1}{6}+3 \cdot \frac{1}{6}+4 \cdot \frac{1}{6}+5 \cdot \frac{1}{6}+6 \cdot \frac{1}{6}=\frac{7}{2}\\
E(X_2)=1 \cdot \frac{1}{6}+2 \cdot \frac{1}{6}+3 \cdot \frac{1}{6}+4 \cdot \frac{1}{6}+5 \cdot \frac{1}{6}+6 \cdot \frac{1}{6}=\frac{7}{2}\\
E(X_1)+E(X_2)=\frac{7}{2}+\frac{7}{2}=7
$$
结果竟然一致，书上是通过[数学归纳法](https://blog.csdn.net/YQXLLWY/article/details/112106627)证明的，但是我在这里就写一下结论

$$
如果 X_i 是 S 上的随机变量，n是正整数，并且 a,b \in N，\\
E(X_1+X_2+X_3 \cdots X_n)=E(X_1)+E(X_2)+E(X_3) \cdots E(X_n)\\
E(a \cdot X+b)=a \cdot E(X)+b
$$

----------

> n次试验的伯努利试验的期望值是 np，其中p是每次试验的中“成功”的概率

[伯努利试验就是试验结果只有2种的事件](https://blog.csdn.net/YQXLLWY/article/details/112596797)

在开始证明前，先证明一个推论：
$$
C(n,k) \cdot k=nC(n-1,k-1)\\
证明：\\
C(n,k) \cdot k=\frac{n!}{(n-k)!k!} \cdot k= \frac{n \cdot (n-1!)}{((n-1)-(k-1))!(k-1)!k} \cdot k= n \frac{(n-1)!}{((n-1)-(k-1))!(k-1)!}=n C(n-1,k-1)
$$


[C(n,k)就是总数为n的k个样本的组合总数](https://blog.csdn.net/YQXLLWY/article/details/112427740)

证明：
$$
E(X)=\sum^{n}_{k=1}k \cdot p(k)\\
=\sum^{n}_{k=1} k \cdot C(n,k) p^k {q}^{(n-k)} \\
=\sum^{n}_{k=1} n \cdot c(n-1,k-1) p^k {q}^{(n-k)}\\
=np \sum^{n}_{k=1} c(n-1,k-1) {p}^{k-1} {q}^{n-k} \\
令 j=k-1 \\
=np \sum^{n}_{k=1} c(n-1,k-1) {p}^{k-1} {q}^{n-k} \\
=np \sum^{n-1}_{j=0} c(n-1,j) {p}^{j} {q}^{n-(j+1)} \\
=np \sum^{n-1}_{j=0} c(n-1,j) {p}^{j} {q}^{n-1-j} \\
=np {(p+q)}^{n-1}=np
$$
其中倒数第二步是因为[二项式定理](https://blog.csdn.net/YQXLLWY/article/details/112417453)

在算法的角度看，**期望值其实就是平均算法复杂度**。但是我看了很久，没有弄懂，所以暂时不深究了。

#### 几何分布

这个证明很鬼扯，但是结论却很简单，所以直接上结论：
$$
如果对于k=1,2,3,4 \cdots n,p(X=k)={(1-p)}^{(k-1)} \cdot p，那么随机变量X具有带参数p的几何分布。
$$
比如投掷骰子，问第n次出现6的概率是多少时：

```mathematica
p(X=1)=1/6
p(X=2)=5/6 * 1/6
p(X=3)=5/6 * 5/6 * 1/6
......
p(X=n)=(1-1/6)^(n-1) * 1/6
```

那么期望值就是：
$$
E(X)=\sum^{n}_{j=1} j \cdot {(1-p)}^{(n-1)} \cdot p
$$
当n趋近于无穷大时，上面的公式可以采用微积分的知识（我忘了）推导为：
$$
E(X)=\frac{1}{p}
$$

#### 独立随机变量

$$
随机变量X和Y在样本空间S上是独立的，则 \\
p(X=r_1 \cap Y=r_2)=p(X=r_1) \cdot p(Y=r_2)
$$

这个很简单，抛开随机变量的定义，就是两个相互独立的事情，其一起发生的概率是各自发生概率的乘积。

在上面的基础上，再加上期望值的概念：
$$
随机变量X和Y在样本空间S上是独立的，则 \\
E(XY)=E(X)\cdot E(Y)
$$
这里书上的证明我感觉是有问题的，也可能是我脑子糊涂了，暂时先记下来，需要的时候再用吧。

## 方差

$$
方差使用 V(X)，或者 \sigma(X) 表示：\\
V(X)=\sum_{s \in S} (X(s)-E(X))^2 \cdot p(s)
$$

在下面说明方差的真实意义之前，先推导一个下面的公式：
$$
V(X)=E(X^2)-E(X)^2
$$
证明：
$$
V(X)=\sum_{s \in S} (X(s)-E(X))^2 \cdot p(s)\\
=\sum_{s \in S} (X(s)^2 - 2X(s)E(X)+E(X)^2) \cdot p(s)\\
=\sum_{s \in S} X(s)^2 \cdot p(s)-\sum_{s \in S} 2X(s)E(X)p(s)+\sum_{s \in S} E(X)^2p(s)\\
=E(X^2)-2E(X) \sum_{s \in S} X(s)p(s)+E(X)^2\\
=E(X^2)-2E(X)E(X)+E(X)^2
=E(X^2)-E(X)
$$
其中一些点的说明：
$$
E(X)是固定值，所以可以单独抽离出来。\\
并且 \sum_{s \in S}p(s)=1。\\
$$
再来证明下面的值：
$$
如果 E(X)=\mu，则 V(X)=E((X-\mu)^2)
$$

证明：
$$
E((X-\mu)^2)=E(X^2-2X \mu+{\mu}^{2})\\
=E(X^2)-E(2X\mu)+E({\mu}^{2})\\
=E(X^2)-2\mu E(X)+{\mu}^{2}\\
=E(X^2)-2\mu \cdot \mu+ \mu \cdot \mu\\
=E(X^2)-{\mu}^2\\
=E(X^2)-(E(X))^2
=V(X)
$$

#### 比安梅公式

$$
对于在样本空间中互相独立的随机变量 X_1,X_2,X_3 \cdots X_n，\\
V(X_1+X_2+X_3 \cdots X_n)=V(X_1)+V(X_2)+V(X_3) \cdots V(X_n)
$$

证明：
$$
V(X+Y)=E((X+Y)^2)-(E(X+Y))^2=E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\
=E(X^2)+2E(XY)+E(Y^2)-(E(X))^2-2E(X)E(Y)-(E(Y))^2\\
=E(X^2)+2E(X)E(Y)+E(Y^2)-(E(X))^2-2E(X)E(Y)-(E(Y))^2\\
=E(X^2)-(E(X))^2+E(Y^2)-(E(Y))^2\\
=V(X)+V(Y)
$$
更多的参数只需要在上面的基础之上进行证明下去。

#### 切比雪夫不等式

$$
r \in N^+，那么 \\
p(|X(s)-E(X)|>=r) <= \frac{V(X)}{r^2}
$$

上面的定义啥意思呢？简单来说就是对于样本空间中的值，其值与期望值差的绝对值大于r的概率，小于方差除以r的平方。

证明：
$$
设事件 A = \{s\in S | |X(s)-E(X)|>=r\}\\
V(X)=\sum_{s \in S}(X(s)-E(X))^2 \cdot p(s)\\
=\sum_{s \in A}(X(s)-E(X))^2 \cdot p(s)+\sum_{s \notin A}(X(s)-E(X))^2 \cdot p(s)\\
首先看 \sum_{s \in A}(X(s)-E(X))^2 \cdot p(s) 的部分，因为是集合A的元素，所以 (X(s)-E(X))>r，所以 \sum_{s \in A}(X(s)-E(X))^2 \cdot p(s) > \sum_{s \in A} r^2 p(s)。\\
\sum_{s \notin A}(X(s)-E(X))^2 \cdot p(s) 肯定大于0，所以\\
V(X)>=\sum_{s \in A} r^2 p(s)，即\\
p(A)<=\frac{V(X)}{r^2}
$$
